{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>\n",
    "    UFRN<br>\n",
    "    Brain Institute<br>\n",
    "    Computational Neurophysiology Lab<br>\n",
    "    </b></p>\n",
    "<p><i>Rodrigo Santiago</i></p>\n",
    "<p> Natal, 2021 </p>\n",
    "<br>\n",
    "<br>\n",
    "<p>DIM0872 — Machine Learning</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Check Point 5 — Ensemble Methods</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Python version:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.13 (default, Feb 20 2021, 21:42:50) \n",
      "[GCC 5.4.0 20160609]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Versions of scientific modules:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy==1.18.2\n",
      "scipy==1.4.1\n",
      "scikit-learn==0.22.2.post1\n"
     ]
    }
   ],
   "source": [
    "!pip3 freeze | grep numpy\n",
    "!pip3 freeze | grep scipy\n",
    "!pip3 freeze | grep scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Loading modules and functions</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import BaggingClassifier,AdaBoostClassifier,StackingClassifier,RandomForestClassifier\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "# from scipy.stats import sem, iqr, shapiro\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Loading database</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BaseOriginal          = pickle.load(open(\"BaseOriginal.txt\",\"rb\"))\n",
    "BaseReduzida1         = pickle.load(open(\"BaseReduzida1.txt\",\"rb\"))\n",
    "BaseReduzida2         = pickle.load(open(\"BaseReduzida2.txt\",\"rb\"))\n",
    "BaseReduzida3         = pickle.load(open(\"BaseReduzida3.txt\",\"rb\"))\n",
    "BaseOriginal_classes  = pickle.load(open(\"BaseOriginal_classes.txt\",\"rb\"))\n",
    "BaseReduzida1_classes = pickle.load(open(\"BaseReduzida1_classes.txt\",\"rb\"))\n",
    "BaseReduzida2_classes = pickle.load(open(\"BaseReduzida2_classes.txt\",\"rb\"))\n",
    "BaseReduzida3_classes = pickle.load(open(\"BaseReduzida3_classes.txt\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = BaseReduzida3\n",
    "y = BaseReduzida3_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Classification</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Number of estimators</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_leaners_list = [10,15,20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Bagging</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Function</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baggingAcc(X,y,model,n_leaners):\n",
    "    \n",
    "    # stratified 10-fold cross-validation\n",
    "    kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "    # accuracy list\n",
    "    bagging_acc = []\n",
    "\n",
    "    bagging = BaggingClassifier(base_estimator=model,n_estimators=n_leaners,\n",
    "                                random_state=0,n_jobs=30,max_samples=0.7)\n",
    "    \n",
    "    for train_index, test_index in kf.split(X,y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        bagging.fit(X_train, y_train)\n",
    "        bagging_acc.append(bagging.score(X_test,y_test))\n",
    "    \n",
    "    return bagging_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Decition Tree</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.89, 0.86, 0.89])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(random_state=0) # no prunning\n",
    "\n",
    "bag_acc_dt = [np.mean(baggingAcc(X,y,model,n_leaners)) for n_leaners in n_leaners_list]\n",
    "\n",
    "np.round(bag_acc_dt,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>k-Nearest Neighbors</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.94, 0.98, 0.95])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = KNeighborsClassifier(n_neighbors=1,weights='distance')\n",
    "\n",
    "X_scaled = minmax_scale(BaseReduzida3)\n",
    "\n",
    "bag_acc_knn = [np.mean(baggingAcc(X_scaled,y,model,n_leaners)) for n_leaners in n_leaners_list]\n",
    "\n",
    "np.round(bag_acc_knn,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Gaussian Naïve Bayes</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.82, 0.82, 0.82])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GaussianNB()\n",
    "\n",
    "bag_acc_nb = [np.mean(baggingAcc(X,y,model,n_leaners)) for n_leaners in n_leaners_list]\n",
    "\n",
    "np.round(bag_acc_nb,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Multi-Layer Perceptron</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.93, 0.92, 0.91])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLPClassifier(hidden_layer_sizes=50,learning_rate_init=.001,max_iter=2000,random_state=0)\n",
    "\n",
    "bag_acc_mlp = [np.mean(baggingAcc(X,y,model,n_leaners)) for n_leaners in n_leaners_list]\n",
    "\n",
    "np.round(bag_acc_mlp,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Compilation</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.89166667, 0.85833333, 0.89166667],\n",
       "       [0.94166667, 0.975     , 0.95      ],\n",
       "       [0.825     , 0.825     , 0.825     ],\n",
       "       [0.93333333, 0.91666667, 0.90833333]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_acc = np.vstack((bag_acc_dt,bag_acc_knn,bag_acc_nb,bag_acc_mlp))\n",
    "bag_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_size = list(np.round(np.mean(bag_acc,axis=1),2))\n",
    "mean_size.append('')\n",
    "d = {'  ': ['DT','kNN','GNB','MLP','Mean (class)'],\n",
    "     '10': [np.round(bag_acc_dt[0],2),\n",
    "            np.round(bag_acc_knn[0],2),\n",
    "            np.round(bag_acc_nb[0],2),\n",
    "            np.round(bag_acc_mlp[0],2),\n",
    "            np.round(np.mean(bag_acc,axis=0)[0],2)],\n",
    "     '15': [np.round(bag_acc_dt[1],2),\n",
    "            np.round(bag_acc_knn[1],2),\n",
    "            np.round(bag_acc_nb[1],2),\n",
    "            np.round(bag_acc_mlp[1],2),\n",
    "            np.round(np.mean(bag_acc,axis=0)[1],2)],\n",
    "     '20': [np.round(bag_acc_dt[2],2),\n",
    "            np.round(bag_acc_knn[2],2),\n",
    "            np.round(bag_acc_nb[2],2),\n",
    "            np.round(bag_acc_mlp[2],2),\n",
    "            np.round(np.mean(bag_acc,axis=0)[2],2)],\n",
    "     'Mean (size)': mean_size}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 10    15    20 Mean (size)\n",
      "           DT  0.89  0.86  0.89        0.88\n",
      "          kNN  0.94  0.98  0.95        0.96\n",
      "          GNB  0.82  0.82  0.82        0.82\n",
      "          MLP  0.93  0.92  0.91        0.92\n",
      " Mean (class)  0.90  0.89  0.89            \n"
     ]
    }
   ],
   "source": [
    "df_bagging = pd.DataFrame(data=d)\n",
    "print(df_bagging.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>AdaBoost</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Function</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boostingAcc(X,y,model,n_leaners,algorithm='SAMME.R'):\n",
    "    \n",
    "    # stratified 10-fold cross-validation\n",
    "    kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "    # accuracy list\n",
    "    boosting_acc = []\n",
    "\n",
    "    boosting = AdaBoostClassifier(base_estimator=model,n_estimators=n_leaners,\n",
    "                                  random_state=0,algorithm=algorithm)\n",
    "    \n",
    "    for train_index, test_index in kf.split(X,y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        boosting.fit(X_train, y_train)\n",
    "        boosting_acc.append(boosting.score(X_test,y_test))\n",
    "    \n",
    "    return boosting_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Decition Tree</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.82, 0.82, 0.82])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(random_state=0) # no prunning\n",
    "\n",
    "boo_acc_dt = [np.mean(boostingAcc(X,y,model,n_leaners)) for n_leaners in n_leaners_list]\n",
    "\n",
    "np.round(boo_acc_dt,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>k-Nearest Neighbors</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "KNeighborsClassifier doesn't support sample_weight.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-8a34eb356048>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminmax_scale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseReduzida3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mboo_acc_knn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboostingAcc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_scaled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_leaners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn_leaners\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mn_leaners_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboo_acc_knn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-8a34eb356048>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminmax_scale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseReduzida3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mboo_acc_knn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboostingAcc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_scaled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_leaners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn_leaners\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mn_leaners_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboo_acc_knn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-3c9b2609ef5e>\u001b[0m in \u001b[0;36mboostingAcc\u001b[0;34m(X, y, model, n_leaners, algorithm)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mboosting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mboosting_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboosting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/ensemble/_weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;31m# Fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/ensemble/_weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;31m# Check parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;31m# Clear any previous fit results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/ensemble/_weight_boosting.py\u001b[0m in \u001b[0;36m_validate_estimator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_fit_parameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_estimator_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sample_weight\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m             raise ValueError(\"%s doesn't support sample_weight.\"\n\u001b[0;32m--> 456\u001b[0;31m                              % self.base_estimator_.__class__.__name__)\n\u001b[0m\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_boost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miboost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: KNeighborsClassifier doesn't support sample_weight."
     ]
    }
   ],
   "source": [
    "model = KNeighborsClassifier(n_neighbors=1,weights='distance')\n",
    "\n",
    "X_scaled = minmax_scale(BaseReduzida3)\n",
    "\n",
    "boo_acc_knn = [np.mean(boostingAcc(X_scaled,y,model,n_leaners)) for n_leaners in n_leaners_list]\n",
    "\n",
    "np.round(boo_acc_knn,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Gaussian Naïve Bayes</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8 , 0.82, 0.82])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GaussianNB()\n",
    "\n",
    "boo_acc_nb = [np.mean(boostingAcc(X,y,model,n_leaners,'SAMME')) for n_leaners in n_leaners_list]\n",
    "\n",
    "np.round(boo_acc_nb,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Multi-Layer Perceptron</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "MLPClassifier doesn't support sample_weight.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-9215551d7557>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLPClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_layer_sizes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mboo_acc_mlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboostingAcc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_leaners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn_leaners\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mn_leaners_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboo_acc_mlp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-9215551d7557>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLPClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_layer_sizes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mboo_acc_mlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboostingAcc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_leaners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn_leaners\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mn_leaners_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboo_acc_mlp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-3c9b2609ef5e>\u001b[0m in \u001b[0;36mboostingAcc\u001b[0;34m(X, y, model, n_leaners, algorithm)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mboosting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mboosting_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboosting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/ensemble/_weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;31m# Fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/ensemble/_weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;31m# Check parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;31m# Clear any previous fit results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/ensemble/_weight_boosting.py\u001b[0m in \u001b[0;36m_validate_estimator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_fit_parameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_estimator_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sample_weight\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m             raise ValueError(\"%s doesn't support sample_weight.\"\n\u001b[0;32m--> 456\u001b[0;31m                              % self.base_estimator_.__class__.__name__)\n\u001b[0m\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_boost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miboost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: MLPClassifier doesn't support sample_weight."
     ]
    }
   ],
   "source": [
    "model = MLPClassifier(hidden_layer_sizes=50,learning_rate_init=.001,max_iter=2000,random_state=0)\n",
    "\n",
    "boo_acc_mlp = [np.mean(boostingAcc(X,y,model,n_leaners)) for n_leaners in n_leaners_list]\n",
    "\n",
    "np.round(boo_acc_mlp,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Compilation</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.825, 0.825, 0.825],\n",
       "       [0.8  , 0.825, 0.825]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# boo_acc = np.vstack((boo_acc_dt,boo_acc_knn,boo_acc_nb,boo_acc_mlp))\n",
    "boo_acc = np.vstack((boo_acc_dt,boo_acc_nb))\n",
    "boo_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_size = list(np.round(np.mean(boo_acc,axis=1),2))\n",
    "mean_size.append('')\n",
    "# d = {'  ': ['DT','kNN','GNB','MLP','Mean (class)'],\n",
    "d = {'  ': ['DT','GNB','Mean (class)'],\n",
    "     '10': [np.round(boo_acc_dt[0],2),\n",
    "#             np.round(boo_acc_knn[0],2),\n",
    "            np.round(boo_acc_nb[0],2),\n",
    "#             np.round(boo_acc_mlp[0],2),\n",
    "            np.round(np.mean(boo_acc,axis=0)[0],2)],\n",
    "     '15': [np.round(boo_acc_dt[1],2),\n",
    "#             np.round(boo_acc_knn[1],2),\n",
    "            np.round(boo_acc_nb[1],2),\n",
    "#             np.round(boo_acc_mlp[1],2),\n",
    "            np.round(np.mean(boo_acc,axis=0)[1],2)],\n",
    "     '20': [np.round(boo_acc_dt[2],2),\n",
    "#             np.round(boo_acc_knn[2],2),\n",
    "            np.round(boo_acc_nb[2],2),\n",
    "#             np.round(boo_acc_mlp[2],2),\n",
    "            np.round(np.mean(boo_acc,axis=0)[2],2)],\n",
    "     'Mean (size)': mean_size}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 10    15    20 Mean (size)\n",
      "           DT  0.82  0.82  0.82        0.82\n",
      "          GNB  0.80  0.82  0.82        0.82\n",
      " Mean (class)  0.81  0.82  0.82            \n"
     ]
    }
   ],
   "source": [
    "df_boosting = pd.DataFrame(data=d)\n",
    "print(df_boosting.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Base learners that support \"sample_weigh\" in the fitting method:</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rsantiago/.local/lib/python3.6/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier\n",
      "BaggingClassifier\n",
      "BernoulliNB\n",
      "CalibratedClassifierCV\n",
      "CategoricalNB\n",
      "ComplementNB\n",
      "DecisionTreeClassifier\n",
      "DummyClassifier\n",
      "ExtraTreeClassifier\n",
      "ExtraTreesClassifier\n",
      "GaussianNB\n",
      "GradientBoostingClassifier\n",
      "LinearSVC\n",
      "LogisticRegression\n",
      "LogisticRegressionCV\n",
      "MultiOutputClassifier\n",
      "MultinomialNB\n",
      "NuSVC\n",
      "Perceptron\n",
      "RandomForestClassifier\n",
      "RidgeClassifier\n",
      "RidgeClassifierCV\n",
      "SGDClassifier\n",
      "SVC\n",
      "StackingClassifier\n",
      "VotingClassifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rsantiago/.local/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "from sklearn.utils.testing import all_estimators\n",
    "for name, clf in all_estimators(type_filter='classifier'):\n",
    "    if 'sample_weight' in inspect.getfullargspec(clf.fit)[0]:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.94, 0.94, 0.94])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "model = LinearSVC(random_state=0,max_iter=5000)\n",
    "\n",
    "boo_acc_dt = [np.mean(boostingAcc(X,y,model,n_leaners,'SAMME')) for n_leaners in n_leaners_list]\n",
    "\n",
    "np.round(boo_acc_dt,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.92, 0.92, 0.92])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "model = Perceptron(random_state=0)\n",
    "\n",
    "boo_acc_dt = [np.mean(boostingAcc(X,y,model,n_leaners,'SAMME')) for n_leaners in n_leaners_list]\n",
    "\n",
    "np.round(boo_acc_dt,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stratified 10-fold cross-validation\n",
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "# accuracy list\n",
    "SVM_acc = []\n",
    "\n",
    "SVM = LinearSVC(random_state=0,max_iter=5000)\n",
    "\n",
    "for train_index, test_index in kf.split(X,y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    SVM.fit(X_train, y_train)\n",
    "    SVM_acc.append(SVM.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9416666666666667"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(SVM_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stratified 10-fold cross-validation\n",
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "# accuracy list\n",
    "perceptron_acc = []\n",
    "\n",
    "perceptron = Perceptron(random_state=0)\n",
    "\n",
    "for train_index, test_index in kf.split(X,y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    perceptron.fit(X_train, y_train)\n",
    "    perceptron_acc.append(perceptron.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptron_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9416666666666667"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(perceptron_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Homogeneous Stacking</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Function</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stackingAcc(X,y,estimators,final_estimator):\n",
    "    \n",
    "    # stratified 10-fold cross-validation\n",
    "    kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "    # accuracy list\n",
    "    stacking_acc = []\n",
    "\n",
    "    stacking = StackingClassifier(estimators=estimators,final_estimator=final_estimator,n_jobs=30)\n",
    "    \n",
    "    for train_index, test_index in kf.split(X,y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        stacking.fit(X_train, y_train)\n",
    "        stacking_acc.append(stacking.score(X_test,y_test))\n",
    "    \n",
    "    return stacking_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Decition Tree</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimators = [\n",
    "    ('dt0', DecisionTreeClassifier(random_state=0,ccp_alpha=0.)),\n",
    "    ('dt1', DecisionTreeClassifier(random_state=4,ccp_alpha=0.05)),\n",
    "    ('dt2', DecisionTreeClassifier(random_state=0,ccp_alpha=0.1)),\n",
    "    ('dt3', DecisionTreeClassifier(random_state=0,ccp_alpha=0.15)),\n",
    "    ('dt4', DecisionTreeClassifier(random_state=0,ccp_alpha=0.2))\n",
    "]\n",
    "\n",
    "final_estimator = DecisionTreeClassifier(random_state=4)\n",
    "\n",
    "sta_acc_dt = np.mean(stackingAcc(X,y,estimators,final_estimator))\n",
    "\n",
    "np.round(sta_acc_dt,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>k-Nearest Neighbors</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled = minmax_scale(BaseReduzida3)\n",
    "\n",
    "estimators = [\n",
    "    ('knn0', KNeighborsClassifier(n_neighbors=1,weights='distance')),\n",
    "    ('knn1', KNeighborsClassifier(n_neighbors=2,weights='distance')),\n",
    "    ('knn2', KNeighborsClassifier(n_neighbors=8,weights='distance')),\n",
    "    ('knn3', KNeighborsClassifier(n_neighbors=16,weights='distance')),\n",
    "    ('knn4', KNeighborsClassifier(n_neighbors=22,weights='distance'))\n",
    "]\n",
    "\n",
    "final_estimator = KNeighborsClassifier(n_neighbors=1,weights='distance')\n",
    "\n",
    "sta_acc_knn = np.mean(stackingAcc(X_scaled,y,estimators,final_estimator))\n",
    "\n",
    "np.round(sta_acc_knn,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Gaussian Naïve Bayes</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimators = [\n",
    "    ('nb0', GaussianNB(var_smoothing=1e-1)),\n",
    "    ('nb1', GaussianNB(var_smoothing=5e-2)),\n",
    "    ('nb2', GaussianNB(var_smoothing=1e-3)),\n",
    "    ('nb3', GaussianNB(var_smoothing=1e-5)),\n",
    "    ('nb4', GaussianNB(var_smoothing=1e-9))\n",
    "]\n",
    "\n",
    "final_estimator = GaussianNB()\n",
    "\n",
    "sta_acc_nb = np.mean(stackingAcc(X,y,estimators,final_estimator))\n",
    "\n",
    "np.round(sta_acc_nb,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Multi-Layer Perceptron</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimators = [\n",
    "    ('mlp0', MLPClassifier(hidden_layer_sizes=50,learning_rate_init=.2,max_iter=2000,random_state=0)),\n",
    "    ('mlp1', MLPClassifier(hidden_layer_sizes=100,learning_rate_init=.2,max_iter=2000,random_state=0)),\n",
    "    ('mlp2', MLPClassifier(hidden_layer_sizes=150,learning_rate_init=.2,max_iter=2000,random_state=0)),\n",
    "    ('mlp3', MLPClassifier(hidden_layer_sizes=200,learning_rate_init=.2,max_iter=2000,random_state=0)),\n",
    "    ('mlp4', MLPClassifier(hidden_layer_sizes=300,learning_rate_init=.2,max_iter=2000,random_state=0))\n",
    "]\n",
    "\n",
    "final_estimator = MLPClassifier(hidden_layer_sizes=500,learning_rate_init=.2,max_iter=2000,random_state=0)\n",
    "\n",
    "sta_acc_mlp = np.mean(stackingAcc(X,y,estimators,final_estimator))\n",
    "\n",
    "np.round(sta_acc_mlp,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Compilation</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.85      ],\n",
       "       [0.95      ],\n",
       "       [0.94166667],\n",
       "       [0.96666667]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sta_acc = np.vstack((sta_acc_dt,sta_acc_knn,sta_acc_nb,sta_acc_mlp))\n",
    "sta_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_size = list(np.round(np.mean(bag_acc,axis=1),2))\n",
    "mean_size.append('')\n",
    "d = {'  ': ['DT','kNN','GNB','MLP','Mean'],\n",
    "     'Stacking': [np.round(sta_acc[0,0],2),\n",
    "                  np.round(sta_acc[1,0],2),\n",
    "                  np.round(sta_acc[2,0],2),\n",
    "                  np.round(sta_acc[3,0],2),\n",
    "                  np.round(np.mean(sta_acc),2)]\n",
    "     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Stacking\n",
      "   DT      0.85\n",
      "  kNN      0.95\n",
      "  GNB      0.94\n",
      "  MLP      0.97\n",
      " Mean      0.93\n"
     ]
    }
   ],
   "source": [
    "df_sta = pd.DataFrame(data=d)\n",
    "print(df_sta.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Heterogeneous Stacking</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>MLP and kNN</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimators = [\n",
    "    ('mlp0', MLPClassifier(hidden_layer_sizes=50,learning_rate_init=.2,max_iter=2000,random_state=0)),\n",
    "    ('mlp1', MLPClassifier(hidden_layer_sizes=100,learning_rate_init=.2,max_iter=2000,random_state=0)),\n",
    "    ('mlp2', MLPClassifier(hidden_layer_sizes=150,learning_rate_init=.2,max_iter=2000,random_state=0)),\n",
    "    ('mlp3', MLPClassifier(hidden_layer_sizes=200,learning_rate_init=.2,max_iter=2000,random_state=0)),\n",
    "    ('mlp4', MLPClassifier(hidden_layer_sizes=300,learning_rate_init=.2,max_iter=2000,random_state=0)),\n",
    "    ('knn0', KNeighborsClassifier(n_neighbors=1,weights='distance')),\n",
    "    ('knn1', KNeighborsClassifier(n_neighbors=2,weights='distance')),\n",
    "    ('knn2', KNeighborsClassifier(n_neighbors=8,weights='distance')),\n",
    "    ('knn3', KNeighborsClassifier(n_neighbors=16,weights='distance')),\n",
    "    ('knn4', KNeighborsClassifier(n_neighbors=22,weights='distance'))\n",
    "]\n",
    "\n",
    "final_estimator = None # logistic regression (default)\n",
    "\n",
    "sta_acc_het_mlp_knn = np.mean(stackingAcc(X,y,estimators,final_estimator))\n",
    "\n",
    "np.round(sta_acc_het_mlp_knn,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>MLP and DT</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimators = [\n",
    "    ('mlp0', MLPClassifier(hidden_layer_sizes=50,learning_rate_init=.2,max_iter=2000,random_state=0)),\n",
    "    ('mlp1', MLPClassifier(hidden_layer_sizes=100,learning_rate_init=.2,max_iter=2000,random_state=0)),\n",
    "    ('mlp2', MLPClassifier(hidden_layer_sizes=150,learning_rate_init=.2,max_iter=2000,random_state=0)),\n",
    "    ('mlp3', MLPClassifier(hidden_layer_sizes=200,learning_rate_init=.2,max_iter=2000,random_state=0)),\n",
    "    ('mlp4', MLPClassifier(hidden_layer_sizes=300,learning_rate_init=.2,max_iter=2000,random_state=0)),\n",
    "    ('dt0', DecisionTreeClassifier(random_state=0,ccp_alpha=0.)),\n",
    "    ('dt1', DecisionTreeClassifier(random_state=4,ccp_alpha=0.05)),\n",
    "    ('dt2', DecisionTreeClassifier(random_state=0,ccp_alpha=0.1)),\n",
    "    ('dt3', DecisionTreeClassifier(random_state=0,ccp_alpha=0.15)),\n",
    "    ('dt4', DecisionTreeClassifier(random_state=0,ccp_alpha=0.2))\n",
    "]\n",
    "\n",
    "final_estimator = None # logistic regression (default)\n",
    "\n",
    "sta_acc_het_mlp_dt = np.mean(stackingAcc(X,y,estimators,final_estimator))\n",
    "\n",
    "np.round(sta_acc_het_mlp_dt,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>kNN and DT</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimators = [\n",
    "    ('knn0', KNeighborsClassifier(n_neighbors=1,weights='distance')),\n",
    "    ('knn1', KNeighborsClassifier(n_neighbors=2,weights='distance')),\n",
    "    ('knn2', KNeighborsClassifier(n_neighbors=8,weights='distance')),\n",
    "    ('knn3', KNeighborsClassifier(n_neighbors=16,weights='distance')),\n",
    "    ('knn4', KNeighborsClassifier(n_neighbors=22,weights='distance')),\n",
    "    ('dt0', DecisionTreeClassifier(random_state=0,ccp_alpha=0.)),\n",
    "    ('dt1', DecisionTreeClassifier(random_state=4,ccp_alpha=0.05)),\n",
    "    ('dt2', DecisionTreeClassifier(random_state=0,ccp_alpha=0.1)),\n",
    "    ('dt3', DecisionTreeClassifier(random_state=0,ccp_alpha=0.15)),\n",
    "    ('dt4', DecisionTreeClassifier(random_state=0,ccp_alpha=0.2))\n",
    "]\n",
    "\n",
    "final_estimator = None # logistic regression (default)\n",
    "\n",
    "sta_acc_het_knn_dt = np.mean(stackingAcc(X,y,estimators,final_estimator))\n",
    "\n",
    "np.round(sta_acc_het_knn_dt,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>MLP, kNN and DT</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimators = [\n",
    "    ('mlp0', MLPClassifier(hidden_layer_sizes=50,learning_rate_init=.2,max_iter=2000,random_state=0)),\n",
    "    ('mlp1', MLPClassifier(hidden_layer_sizes=100,learning_rate_init=.2,max_iter=2000,random_state=0)),\n",
    "    ('mlp2', MLPClassifier(hidden_layer_sizes=150,learning_rate_init=.2,max_iter=2000,random_state=0)),\n",
    "    ('mlp3', MLPClassifier(hidden_layer_sizes=200,learning_rate_init=.2,max_iter=2000,random_state=0)),\n",
    "    ('mlp4', MLPClassifier(hidden_layer_sizes=300,learning_rate_init=.2,max_iter=2000,random_state=0)),\n",
    "    ('knn0', KNeighborsClassifier(n_neighbors=1,weights='distance')),\n",
    "    ('knn1', KNeighborsClassifier(n_neighbors=2,weights='distance')),\n",
    "    ('knn2', KNeighborsClassifier(n_neighbors=8,weights='distance')),\n",
    "    ('knn3', KNeighborsClassifier(n_neighbors=16,weights='distance')),\n",
    "    ('knn4', KNeighborsClassifier(n_neighbors=22,weights='distance')),\n",
    "    ('dt0', DecisionTreeClassifier(random_state=0,ccp_alpha=0.)),\n",
    "    ('dt1', DecisionTreeClassifier(random_state=4,ccp_alpha=0.05)),\n",
    "    ('dt2', DecisionTreeClassifier(random_state=0,ccp_alpha=0.1)),\n",
    "    ('dt3', DecisionTreeClassifier(random_state=0,ccp_alpha=0.15)),\n",
    "    ('dt4', DecisionTreeClassifier(random_state=0,ccp_alpha=0.2))\n",
    "]\n",
    "\n",
    "final_estimator = None # logistic regression (default)\n",
    "\n",
    "sta_acc_het_mlp_knn_dt = np.mean(stackingAcc(X,y,estimators,final_estimator))\n",
    "\n",
    "np.round(sta_acc_het_mlp_knn_dt,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Random Forest</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Function</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomforestAcc(X,y,n_estimators,seed):\n",
    "    \n",
    "    # stratified 10-fold cross-validation\n",
    "    kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "    # accuracy list\n",
    "    rf_acc_list = []\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=n_estimators,n_jobs=30,random_state=seed)\n",
    "    \n",
    "    for train_index, test_index in kf.split(X,y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        rf.fit(X_train, y_train)\n",
    "        rf_acc_list.append(rf.score(X_test,y_test))\n",
    "    \n",
    "    return rf_acc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Accuracy for 10, 15 and 20 learners</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8 , 0.82, 0.83])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_acc = [np.mean(randomforestAcc(X,y,n_leaners,seed=seed))\n",
    "          for n_leaners in n_leaners_list\n",
    "          for seed in np.arange(30)]\n",
    "\n",
    "np.round(np.mean(np.split(np.array(rf_acc),3),axis=1),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Bagging with bootstrap of the features</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Function</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baggingAcc(X,y,model,n_leaners,max_features=0.9):\n",
    "    \n",
    "    # stratified 10-fold cross-validation\n",
    "    kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "    # accuracy list\n",
    "    bagging_acc = []\n",
    "\n",
    "    bagging = BaggingClassifier(base_estimator=model,n_estimators=n_leaners,\n",
    "                                bootstrap_features=True,max_features=max_features,\n",
    "                                random_state=0,n_jobs=30,max_samples=0.7)\n",
    "    \n",
    "    for train_index, test_index in kf.split(X,y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        bagging.fit(X_train, y_train)\n",
    "        bagging_acc.append(bagging.score(X_test,y_test))\n",
    "    \n",
    "    return bagging_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Decition Tree</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.92, 0.89, 0.92])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(random_state=0) # no prunning\n",
    "\n",
    "bag_acc_dt = [np.mean(baggingAcc(X,y,model,n_leaners,max_features=0.8)) for n_leaners in n_leaners_list]\n",
    "\n",
    "np.round(bag_acc_dt,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>k-Nearest Neighbors</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.  , 0.95, 0.88])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = KNeighborsClassifier(n_neighbors=1,weights='distance')\n",
    "\n",
    "X_scaled = minmax_scale(BaseReduzida3)\n",
    "\n",
    "bag_acc_knn = [np.mean(baggingAcc(X_scaled,y,model,n_leaners,max_features=0.8)) for n_leaners in n_leaners_list]\n",
    "\n",
    "np.round(bag_acc_knn,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Gaussian Naïve Bayes</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.78, 0.81, 0.83])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GaussianNB()\n",
    "\n",
    "bag_acc_nb = [np.mean(baggingAcc(X,y,model,n_leaners,max_features=0.8)) for n_leaners in n_leaners_list]\n",
    "\n",
    "np.round(bag_acc_nb,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Multi-Layer Perceptron</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.92, 0.84, 0.9 ])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLPClassifier(hidden_layer_sizes=50,learning_rate_init=.2,max_iter=2000,random_state=0)\n",
    "\n",
    "bag_acc_mlp = [np.mean(baggingAcc(X,y,model,n_leaners,max_features=0.8)) for n_leaners in n_leaners_list]\n",
    "\n",
    "np.round(bag_acc_mlp,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Compilation</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.91666667, 0.89166667, 0.925     ],\n",
       "       [1.        , 0.95      , 0.88333333],\n",
       "       [0.775     , 0.80833333, 0.83333333],\n",
       "       [0.925     , 0.84166667, 0.9       ]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_acc = np.vstack((bag_acc_dt,bag_acc_knn,bag_acc_nb,bag_acc_mlp))\n",
    "bag_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_size = list(np.round(np.mean(bag_acc,axis=1),2))\n",
    "mean_size.append('')\n",
    "d = {'  ': ['DT','kNN','GNB','MLP','Mean (class)'],\n",
    "     '10': [np.round(bag_acc_dt[0],2),\n",
    "            np.round(bag_acc_knn[0],2),\n",
    "            np.round(bag_acc_nb[0],2),\n",
    "            np.round(bag_acc_mlp[0],2),\n",
    "            np.round(np.mean(bag_acc,axis=0)[0],2)],\n",
    "     '15': [np.round(bag_acc_dt[1],2),\n",
    "            np.round(bag_acc_knn[1],2),\n",
    "            np.round(bag_acc_nb[1],2),\n",
    "            np.round(bag_acc_mlp[1],2),\n",
    "            np.round(np.mean(bag_acc,axis=0)[1],2)],\n",
    "     '20': [np.round(bag_acc_dt[2],2),\n",
    "            np.round(bag_acc_knn[2],2),\n",
    "            np.round(bag_acc_nb[2],2),\n",
    "            np.round(bag_acc_mlp[2],2),\n",
    "            np.round(np.mean(bag_acc,axis=0)[2],2)],\n",
    "     'Mean (size)': mean_size}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 10    15    20 Mean (size)\n",
      "           DT  0.92  0.89  0.92        0.91\n",
      "          kNN  1.00  0.95  0.88        0.94\n",
      "          GNB  0.78  0.81  0.83        0.81\n",
      "          MLP  0.92  0.84  0.90        0.89\n",
      " Mean (class)  0.90  0.87  0.89            \n"
     ]
    }
   ],
   "source": [
    "df_bagging = pd.DataFrame(data=d)\n",
    "print(df_bagging.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
